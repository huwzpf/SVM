{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f17083",
   "metadata": {},
   "source": [
    "# Sequential Minimal Optimization algotihm for SVM\n",
    "\n",
    "A SMO class is created for easier debugging and in order to avoid passing all the parameters to every method.\n",
    "Class variables incude $ \\alpha $ vector, $ b $, $ x $ and $ y $ vectors, $ C $ regularization parameter and $ \\gamma $ parameter for kernel function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef953c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def __init__(self, x, y, c, gamma):\n",
    "        self.b = 0\n",
    "        self.alpha = np.zeros((x.shape[0], 1))\n",
    "        self.features = x\n",
    "        self.labels = y.reshape(len(y), 1)\n",
    "        self.c = c\n",
    "        self.gamma = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ab99b3",
   "metadata": {},
   "source": [
    "SMO is a dual optimization problem stated as:\n",
    "$$ max_\\alpha \\quad \\quad W(\\alpha) \\ = \\  \\sum_{i = 1}^{m}\\alpha_i  - \\frac{1}{2} \\sum_{i = 1}^{m} \\sum_{j = 1}^{m} y^{(i)}y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle  $$\n",
    "\n",
    "subject to:\n",
    "$$ 0 \\leq \\alpha_i \\leq C \\quad and \\quad \\sum_{i = 1}^{m}\\alpha_i y^{(i)} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54284a0",
   "metadata": {},
   "source": [
    "A hypothesis function is given by linear classifier formula :\n",
    "\n",
    "\\begin{equation}\n",
    "   f(x) = \\omega^Tx+b =   \\sum_{i=1}^m \\alpha_i y^{(i)} \\langle x^{(i)}, x \\rangle+ b\n",
    "\\end{equation}\n",
    " \n",
    " So when $ \\alpha, X  =  \\langle x^{(i)}, x \\rangle $ and $y$ are vectors we get:\n",
    " $$ \\alpha ./  y\\cdot  X^T + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "433debbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def hypothesis(self, x):\n",
    "        return np.transpose(np.multiply(self.alpha, self.labels))\\\n",
    "                  .dot(np.array([self.kernel_function(a, x, self.gamma) for a in self.features])\n",
    "                       .reshape(self.features.shape[0], 1)) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5548166",
   "metadata": {},
   "source": [
    "A Gaussian kernel is used given by: $$ \\langle x^{(i)}, x^{(j)} \\rangle = exp(\\frac{|| x_i - x_j ||^2}{-\\gamma}) = exp(\\frac{\\sqrt{(x_i - x_j)^T \\cdot (x_i - x_j)}^2}{-\\gamma}) = exp(\\frac{(x_i - x_j)^T \\cdot (x_i - x_j)}{-\\gamma}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba6c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def kernel_function(x, z, gamma):\n",
    "        k = np.transpose(x-z).dot(x-z)/-gamma\n",
    "        return np.exp(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed3968",
   "metadata": {},
   "source": [
    "After choosing multipliers $ \\alpha_i $ and $ \\alpha_j $ to optimize we need to compute constrains on them, such that \n",
    "$$ L \\leq \\alpha \\leq H $$\n",
    "in order to satisfy \n",
    "$$ 0 \\leq \\alpha \\leq C $$\n",
    "which are given by :\n",
    "\n",
    "  if $ \\quad y^{(i)} = y^{(j)}, \\quad  L = max(0 , \\alpha_j +\\alpha_i - C), \\quad H = min(C, \\alpha_j + \\alpha_i) $\n",
    "  \n",
    "  otherwise $ \\quad  L = max(0 , \\alpha_j - \\alpha_i), \\quad H = min(C, C + \\alpha_j - \\alpha_i) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869548f",
   "metadata": {},
   "outputs": [],
   "source": [
    " def calculate_constrains(self, i, j):\n",
    "        # return L, H\n",
    "        if self.labels[i] == self.labels[j]:\n",
    "            return max(0.0, float(self.alpha[j] + self.alpha[i] - self.c)), min(self.c, float(self.alpha[j] + self.alpha[i]))\n",
    "        else:\n",
    "            return max(0.0, float(self.alpha[j] - self.alpha[i])), min(self.c, float(self.c + self.alpha[j] - self.alpha[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9775299e",
   "metadata": {},
   "source": [
    "Next we want to find $ \\alpha_j $ so as to maximize objective function \n",
    "\n",
    "Update rule for $ \\alpha_j $ is :\n",
    "\n",
    "$$ \\alpha_j \\ -= \\frac{y^{(j)}(E_i - E_j)}{\\eta} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9beea88",
   "metadata": {},
   "source": [
    "We need to calculate errors given by:\n",
    "$$ E_k = f(x^{(k)}) - y^{(k)}  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3661fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calculate_error(self, i):\n",
    "        return self.hypothesis(self.features[i]) - self.labels[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f63edb",
   "metadata": {},
   "source": [
    "And $ \\eta $ given by:\n",
    "$$ \\eta = 2\\langle x^{(i)}, x^{(j)} \\rangle - \\langle x^{(i)}, x^{(i)} \\rangle -\\langle x^{(j)}, x^{(j)} \\rangle $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a644f6",
   "metadata": {},
   "outputs": [],
   "source": [
    " def calculate_eta(self, i, j):\n",
    "        return 2 * self.kernel_function(self.features[i], self.features[j], self.gamma) \\\n",
    "               - self.kernel_function(self.features[i], self.features[i], self.gamma) -\\\n",
    "               self.kernel_function(self.features[j], self.features[j], self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa98c6b4",
   "metadata": {},
   "source": [
    "Clip $\\alpha_j$ to bounds defined above if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b73059",
   "metadata": {},
   "source": [
    "And finally update $ \\alpha_i $ value :\n",
    "$$ \\alpha_i \\ += y^{(i)}y^{(j)}(\\alpha_j^{(old)} - \\alpha_j) $$\n",
    "where $ \\alpha_j^{(old)} $ is a value before optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def update_alpha_j(self, i, j):\n",
    "        alpha_j_old = self.alpha[j]\n",
    "        self.alpha[j] -= float(self.labels[j] * (self.calculate_error(i) - self.calculate_error(j))) / float(self.calculate_eta(i,j))\n",
    "        L, H = self.calculate_constrains(i, j)\n",
    "\n",
    "        if self.alpha[j] > H:\n",
    "            self.alpha[j] = H\n",
    "        elif self.alpha[j] < L:\n",
    "            self.alpha[j] = L"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f321a56",
   "metadata": {},
   "source": [
    "Finally we select the threshold $ b $ such that KKT conditions are satisfied for $ i $ -th and $ j $ -th example\n",
    "\n",
    "If, after optimization, $ 0 < \\alpha_i < C $\n",
    "then the following threshold $ b_1 $ is valid, since it forces the SVM to output $ y^{(i)}$ when the input is $x^{(i)} $\n",
    "\n",
    "\n",
    "$$ b_1 = b - E_i -y^{(i)}(\\alpha_i - \\alpha_i^{(old)}) \\langle x^{(i)}, x^{(i)} \\rangle -y^{(j)}(\\alpha_j - \\alpha_j^{(old)}) \\langle x^{(i)}, x^{(j)} \\rangle $$\n",
    "\n",
    "\n",
    "Similarly, the following threshold $b_2$ is valid if $ 0 < \\alpha_j < C $\n",
    "\n",
    "\n",
    "$$ b_2 = b - E_j -y^{(i)}(\\alpha_i - \\alpha_i^{(old)}) \\langle x^{(i)}, x^{(j)} \\rangle -y^{(j)}(\\alpha_j - \\alpha_j^{(old)}) \\langle x^{(j)}, x^{(i)} \\rangle $$\n",
    "\n",
    "\n",
    "If both $ 0 < \\alpha_i < C$ and $ 0 < \\alpha_j < C $ then both these thresholds are valid, and they will be equal, so $ b = \\frac{(b_1 + b_2)}{2} $ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d97498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def calculate_b(self, i, j, a_i_old, a_j_old):\n",
    "        b_1 = self.b - self.calculate_error(i) - self.labels[i] * (self.alpha[i] - a_i_old) * \\\n",
    "              self.kernel_function(self.features[i], self.features[i], self.gamma) - self.labels[j] *\\\n",
    "              (self.alpha[j] - a_j_old) * self.kernel_function(self.features[i], self.features[j], self.gamma)\n",
    "\n",
    "        b_2 = self.b - self.calculate_error(j) - self.labels[i] * (self.alpha[i] - a_i_old) *\\\n",
    "              self.kernel_function(self.features[i], self.features[j], self.gamma) - self.labels[j] *\\\n",
    "              (self.alpha[j] - a_j_old) * self.kernel_function(self.features[j], self.features[i], self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2fbe30",
   "metadata": {},
   "source": [
    "Additionally a method for selecting $ \\alpha_j $ is needed. The simplest solution is choosing it randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5988f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def select_j(self, i):\n",
    "        random.seed(time.time())\n",
    "        j = random.randint(0, self.features.shape[0]-1)\n",
    "\n",
    "        while i == j:\n",
    "            j = random.randint(0, self.features.shape[0]-1)\n",
    "\n",
    "        return j"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569bb0c1",
   "metadata": {},
   "source": [
    "Main function train takes tolerance and maximum number of iterations possible without change to $ \\alpha $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9c3f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def train(self, tol, max_n):\n",
    "        n = 0\n",
    "        while n < max_n:\n",
    "            changed_alphas = 0\n",
    "            for i in range(self.features.shape[0]):\n",
    "                E_i = self.calculate_error(i)\n",
    "                if (self.labels[i] * E_i < -tol and self.alpha[i] < self.c) or\\\n",
    "                        (self.labels[i] * E_i > tol and self.alpha[i] > 0):\n",
    "                    j = self.select_j(i)\n",
    "                    old_a_i = self.alpha[i]\n",
    "                    old_a_j = self.alpha[j]\n",
    "                    l, h = self.calculate_constrains(i, j)\n",
    "                    if l == h:\n",
    "                        continue\n",
    "                    eta = self.calculate_eta(i, j)\n",
    "                    if eta > 0:\n",
    "                        continue\n",
    "                    self.update_alpha_j(i, j)\n",
    "                    if abs(self.alpha[j] - old_a_j) < 10 ** -5:\n",
    "                        continue\n",
    "                    # update alpha_i\n",
    "                    self.alpha[i] += self.labels[i] * self.labels[j] * (old_a_j - self.alpha[j])\n",
    "                    self.calculate_b(i, j, old_a_i, old_a_j)\n",
    "                    changed_alphas += 1\n",
    "            if changed_alphas == 0 :\n",
    "                n += 1\n",
    "            else:\n",
    "                n = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba57a1bd",
   "metadata": {},
   "source": [
    "Plot of decision boundary found by algorrithm with parameters : \n",
    "$ tol = 0.0000005 \\quad max\\_iter = 50 \\quad C = 0.1 \\quad \\gamma = 100 \\quad $ \n",
    "is shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a235e7f",
   "metadata": {},
   "source": [
    "![decision_boundary](plot_SVM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f301bdc8",
   "metadata": {},
   "source": [
    "SMO algorithm is based on a simplified version overview from http://chubakbidpaa.com/assets/pdf/smo.pdf\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
